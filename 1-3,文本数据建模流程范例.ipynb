{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37dfe75a",
   "metadata": {},
   "source": [
    "# 1-3,文本数据建模流程范例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87488d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#mac系统上pytorch和matplotlib在jupyter中同时跑需要更改环境变量\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfe3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pip安装gensim库，用于自然语言处理和主题建模等任务\n",
    "!pip install gensim\n",
    "\n",
    "# 使用pip安装torchkeras库，它是一个用于PyTorch深度学习框架的高级封装，简化了模型训练和评估的流程\n",
    "!pip install torchkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95288a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gensim\n",
    "import torchkeras\n",
    "\n",
    "print(\"torch.__version__ = \", torch.__version__)\n",
    "print(\"gensim.__version__ = \", gensim.__version__)\n",
    "print(\"torchkeras.__version__ = \", torchkeras.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be150a8a",
   "metadata": {},
   "source": [
    "### 一，准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21091b",
   "metadata": {},
   "source": [
    "imdb数据集的目标是根据电影评论的文本内容预测评论的情感标签。\n",
    "\n",
    "训练集有20000条电影评论文本，测试集有5000条电影评论文本，其中正面评论和负面评论都各占一半。\n",
    "\n",
    "文本数据预处理较为繁琐，包括文本切词，构建词典，编码转换，序列填充，构建数据管道等等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a8aa6",
   "metadata": {},
   "source": [
    "此处使用gensim中的词典工具并自定义Dataset。\n",
    "\n",
    "下面进行演示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9132822a",
   "metadata": {},
   "source": [
    "![](./data/电影评论.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5fd57-44e7-4f64-9743-673d6b090f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入pandas库，用于数据处理和操作\n",
    "import pandas as pd\n",
    "\n",
    "# 定义最大文本长度，每个样本保留200个词的长度\n",
    "MAX_LEN = 200\n",
    "\n",
    "# 定义批量大小，每个批次包含20个样本\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# 使用pandas库读取训练数据集，数据以制表符分隔，没有列名，分别存储在dftrain中\n",
    "dftrain = pd.read_csv(\"./eat_pytorch_datasets/imdb/train.tsv\", sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "\n",
    "# 使用pandas库读取验证数据集，数据以制表符分隔，没有列名，分别存储在dfval中\n",
    "dfval = pd.read_csv(\"./eat_pytorch_datasets/imdb/test.tsv\", sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca1d76-7644-4d1d-b609-08523f3d8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入gensim库中的corpora模块，用于构建词典和文本处理\n",
    "from gensim import corpora\n",
    "\n",
    "# 导入string库，用于文本处理，去除标点符号\n",
    "import string\n",
    "\n",
    "\n",
    "# 1. 文本切词函数\n",
    "def textsplit(text):\n",
    "    # 创建一个标点符号转换器，用于去除文本中的标点符号\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    # 将文本按照空格切分成词汇列表，并去除标点符号\n",
    "    words = text.translate(translator).split(' ')\n",
    "    return words\n",
    "\n",
    "\n",
    "# 2. 构建词典\n",
    "# 使用corpora库中的Dictionary类构建词典\n",
    "vocab = corpora.Dictionary((textsplit(text) for text in dftrain['text']))\n",
    "\n",
    "# 过滤掉出现频率过低或过高的词汇\n",
    "vocab.filter_extremes(no_below=5, no_above=5000)\n",
    "\n",
    "# 添加特殊的标记词汇，如'<pad>'和'<unk>'，并更新词典\n",
    "special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "vocab.patch_with_special_tokens(special_tokens)\n",
    "\n",
    "# 获取词典的大小\n",
    "vocab_size = len(vocab.token2id)\n",
    "print('vocab_size = ', vocab_size)\n",
    "\n",
    "\n",
    "# 3. 序列填充函数\n",
    "def pad(seq, max_length, pad_value=0):\n",
    "    n = len(seq)\n",
    "    # 将序列扩展到指定的最大长度，并用pad_value填充\n",
    "    result = seq + [pad_value] * max_length\n",
    "    return result[:max_length]\n",
    "\n",
    "\n",
    "# 4. 编码转换函数\n",
    "def text_pipeline(text):\n",
    "    # 将文本切分成词汇，并使用词典将词汇编码为整数序列\n",
    "    tokens = vocab.doc2idx(textsplit(text))\n",
    "\n",
    "    # 将未在词典中找到的词汇替换为'<unk>'的编码\n",
    "    tokens = [x if x > 0 else special_tokens['<unk>'] for x in tokens]\n",
    "\n",
    "    # 对编码后的序列进行填充，使其达到最大长度\n",
    "    result = pad(tokens, MAX_LEN, special_tokens['<pad>'])\n",
    "    return result\n",
    "\n",
    "\n",
    "# 测试text_pipeline函数，将文本转换为编码序列\n",
    "print(text_pipeline(\"this is an example!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6675f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8338e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入PyTorch中的数据集和数据加载器相关模块\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# 创建自定义的ImdbDataset类，用于加载IMDb电影评论数据集\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集的长度，即样本的数量\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引获取单个样本的文本和标签\n",
    "        text = self.df[\"text\"].iloc[index]\n",
    "        label = torch.tensor([self.df[\"label\"].iloc[index]]).float()\n",
    "\n",
    "        # 使用之前定义的text_pipeline函数将文本转换为编码序列\n",
    "        tokens = torch.tensor(text_pipeline(text)).int()\n",
    "\n",
    "        # 返回编码序列和对应的标签\n",
    "        return tokens, label\n",
    "\n",
    "\n",
    "# 创建训练集和验证集的数据集实例\n",
    "ds_train = ImdbDataset(dftrain)  # 训练集\n",
    "ds_val = ImdbDataset(dfval)  # 验证集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698296a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建训练集和验证集的数据加载器，用于批量加载数据\n",
    "# ds_train: 训练集的数据集对象\n",
    "# ds_val: 验证集的数据集对象\n",
    "# batch_size: 每个批次的样本数量\n",
    "# shuffle: 是否在每个epoch中打乱数据顺序\n",
    "\n",
    "# 创建训练集的数据加载器，每个批次包含50个样本，并在每个epoch中打乱数据顺序\n",
    "dl_train = DataLoader(ds_train, batch_size=50, shuffle=True)\n",
    "\n",
    "# 创建验证集的数据加载器，每个批次包含50个样本，不打乱数据顺序（用于评估模型性能）\n",
    "dl_val = DataLoader(ds_val, batch_size=50, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代训练集数据加载器 dl_train 中的第一个批次数据\n",
    "for features, labels in dl_train:\n",
    "    # features: 一个批次的特征数据，通常是文本编码序列\n",
    "    # labels: 对应的标签数据，通常是与特征数据相关的目标标签\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8971bba1",
   "metadata": {},
   "source": [
    "### 二，定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d014a534",
   "metadata": {},
   "source": [
    "使用Pytorch通常有三种方式构建模型：使用nn.Sequential按层顺序构建模型，继承nn.Module基类构建自定义模型，继承nn.Module基类构建模型并辅助应用模型容器(nn.Sequential,nn.ModuleList,nn.ModuleDict)进行封装。\n",
    "\n",
    "此处选择使用第三种方式进行构建。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b42dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入PyTorch库\n",
    "import torch\n",
    "\n",
    "# 导入PyTorch中的神经网络模块\n",
    "from torch import nn\n",
    "\n",
    "# 设置随机种子，以确保随机数生成的结果可重复\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37193036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建自定义神经网络模型Net，继承自nn.Module\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 创建一个词嵌入层（Embedding Layer），用于将整数编码的词汇转换为词嵌入向量\n",
    "        # num_embeddings: 词汇表的大小\n",
    "        # embedding_dim: 词嵌入向量的维度\n",
    "        # padding_idx: 设置padding_idx参数后，在训练过程中将填充的token始终赋值为0向量\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=3, padding_idx=0)\n",
    "\n",
    "        # 创建一个卷积神经网络模块\n",
    "        self.conv = nn.Sequential()\n",
    "        self.conv.add_module(\"conv_1\", nn.Conv1d(in_channels=3, out_channels=16, kernel_size=5))\n",
    "        self.conv.add_module(\"pool_1\", nn.MaxPool1d(kernel_size=2))\n",
    "        self.conv.add_module(\"relu_1\", nn.ReLU())\n",
    "        self.conv.add_module(\"conv_2\", nn.Conv1d(in_channels=16, out_channels=128, kernel_size=2))\n",
    "        self.conv.add_module(\"pool_2\", nn.MaxPool1d(kernel_size=2))\n",
    "        self.conv.add_module(\"relu_2\", nn.ReLU())\n",
    "\n",
    "        # 创建一个全连接神经网络模块\n",
    "        self.dense = nn.Sequential()\n",
    "        self.dense.add_module(\"flatten\", nn.Flatten())\n",
    "        self.dense.add_module(\"linear\", nn.Linear(6144, 1))  # 6144是全连接层输入维度，1是输出维度（二分类）\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入x是文本数据的编码序列\n",
    "        # 通过词嵌入层将编码序列转换为词嵌入向量，并进行维度转置\n",
    "        x = self.embedding(x).transpose(1, 2)\n",
    "\n",
    "        # 将词嵌入向量输入卷积神经网络模块中进行卷积、池化和ReLU激活操作\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # 将卷积后的结果输入全连接神经网络模块中，得到最终输出y\n",
    "        y = self.dense(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "# 创建Net类的实例，即神经网络模型\n",
    "net = Net()\n",
    "\n",
    "# 打印网络结构\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f34cc2",
   "metadata": {},
   "source": [
    "卷积神经网络（Convolutional Neural Network，CNN）和全连接神经网络（Fully Connected Neural Network，FCN）是深度学习中常用的两种神经网络架构，它们包含不同类型的层，每种层都有其特定的作用。\n",
    "\n",
    "### 卷积神经网络（CNN）：\n",
    "\n",
    "1. **卷积层（Convolutional Layer）**：\n",
    "   - 作用：用于提取输入数据中的特征。卷积操作通过滤波器（卷积核）在输入数据上滑动，执行局部感知，从输入中提取特定的特征。\n",
    "   - 设置原因：卷积层通过局部连接和共享权重的方式，有效地捕捉图像、文本等数据中的空间局部关系，减少了参数数量，提高了特征提取的效率。\n",
    "\n",
    "2. **池化层（Pooling Layer）**：\n",
    "   - 作用：用于降低特征图的空间分辨率，减少计算量，并提高模型对平移不变性的学习能力。\n",
    "   - 设置原因：池化层可以减小数据的尺寸，同时保留关键信息，有助于降低模型的复杂度和提高模型的泛化能力。\n",
    "\n",
    "3. **ReLU激活层（Rectified Linear Unit Activation Layer）**：\n",
    "   - 作用：引入非线性，增强模型的表达能力，使模型能够学习复杂的特征。\n",
    "   - 设置原因：ReLU激活函数能够解决梯度消失问题，同时计算速度较快。\n",
    "\n",
    "### 全连接神经网络（FCN）：\n",
    "\n",
    "1. **全连接层（Fully Connected Layer）**：\n",
    "   - 作用：每个神经元与前一层的所有神经元相连接，用于学习全局特征和实现分类、回归等任务。\n",
    "   - 设置原因：全连接层可以对所有输入进行加权组合，具有较强的表达能力，适用于各种复杂任务。\n",
    "\n",
    "2. **激活层（Activation Layer）**：\n",
    "   - 作用：引入非线性，增强模型的非线性拟合能力。\n",
    "   - 设置原因：激活层可以使神经网络模型具有非线性映射能力，可以学习到更复杂的函数关系。\n",
    "\n",
    "总结起来，卷积神经网络通过卷积层、池化层和ReLU激活层构建了一个逐渐提取特征的层次结构，适用于处理具有空间局部关系的数据，如图像和文本。全连接神经网络则通过全连接层和激活层构建了一个密集连接的层次结构，适用于学习全局特征和执行各种复杂任务。这些不同层的组合和设计是为了使模型能够有效地捕捉数据的特征并实现不同的机器学习任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57016b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 导入torchkeras库中的summary函数\n",
    "from torchkeras import summary\n",
    "\n",
    "# 使用summary函数生成神经网络模型net的摘要信息\n",
    "# net: 要生成摘要信息的神经网络模型\n",
    "# input_data: 输入数据的示例，用于确定模型的输入维度\n",
    "summary(net, input_data=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc09084",
   "metadata": {},
   "source": [
    "### 三，训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d476c85",
   "metadata": {},
   "source": [
    "训练Pytorch通常需要用户编写自定义训练循环，训练循环的代码风格因人而异。\n",
    "\n",
    "有3类典型的训练循环代码风格：脚本形式训练循环，函数形式训练循环，类形式训练循环。\n",
    "\n",
    "此处介绍一种较通用的仿照Keras风格的类形式的训练循环。\n",
    "\n",
    "该训练循环的代码也是torchkeras库的核心代码。\n",
    "\n",
    "torchkeras详情:  https://github.com/lyhue1991/torchkeras \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7436987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库和模块\n",
    "import sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# 定义一个用于打印日志的函数\n",
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\" + \"==========\" * 8 + \"%s\" % nowtime)\n",
    "    print(str(info) + \"\\n\")\n",
    "\n",
    "\n",
    "# 定义一个用于执行单个训练或验证步骤的类\n",
    "class StepRunner:\n",
    "    # 初始化方法，接受神经网络模型、损失函数、度量指标字典、优化器和学习率调度器等参数\n",
    "    def __init__(self, net, loss_fn, metrics_dict=None, optimizer=None, lr_scheduler=None, stage=\"train\"):\n",
    "        self.net, self.loss_fn, self.metrics_dict, self.stage = net, loss_fn, metrics_dict, stage\n",
    "        self.optimizer, self.lr_scheduler = optimizer, lr_scheduler\n",
    "\n",
    "    def __call__(self, features, labels):\n",
    "        # 前向传播，计算损失\n",
    "        preds = self.net(features)\n",
    "        loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        # 反向传播和优化（仅在训练阶段执行）\n",
    "        if self.optimizer is not None and self.stage == \"train\":\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        # 计算度量指标\n",
    "        step_metrics = {self.stage + \"_\" + name: metric_fn(preds, labels).item()\n",
    "                        for name, metric_fn in self.metrics_dict.items()}\n",
    "        return loss.item(), step_metrics\n",
    "\n",
    "\n",
    "# 定义一个用于执行一个训练或验证周期的类\n",
    "class EpochRunner:\n",
    "    # 初始化方法，接受StepRunner实例作为参数\n",
    "    def __init__(self, steprunner):\n",
    "        self.steprunner = steprunner\n",
    "        self.stage = steprunner.stage\n",
    "        self.steprunner.net.train() if self.stage == \"train\" else self.steprunner.net.eval()\n",
    "\n",
    "    def __call__(self, dataloader):\n",
    "        total_loss, step = 0, 0\n",
    "        loop = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "        for i, batch in loop:\n",
    "            if self.stage == \"train\":\n",
    "                loss, step_metrics = self.steprunner(*batch)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    loss, step_metrics = self.steprunner(*batch)\n",
    "            step_log = dict({self.stage + \"_loss\": loss}, **step_metrics)\n",
    "\n",
    "            total_loss += loss\n",
    "            step += 1\n",
    "            if i != len(dataloader) - 1:\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_loss = total_loss / step\n",
    "                epoch_metrics = {self.stage + \"_\" + name: metric_fn.compute().item()\n",
    "                                 for name, metric_fn in self.steprunner.metrics_dict.items()}\n",
    "                epoch_log = dict({self.stage + \"_loss\": epoch_loss}, **epoch_metrics)\n",
    "                loop.set_postfix(**epoch_log)\n",
    "\n",
    "                for name, metric_fn in self.steprunner.metrics_dict.items():\n",
    "                    metric_fn.reset()\n",
    "        return epoch_log\n",
    "\n",
    "\n",
    "# 定义一个用于构建深度学习模型的类\n",
    "class KerasModel(torch.nn.Module):\n",
    "    # 初始化方法，接受神经网络模型、损失函数、度量指标字典、优化器和学习率调度器等参数\n",
    "    def __init__(self, net, loss_fn, metrics_dict=None, optimizer=None, lr_scheduler=None):\n",
    "        super().__init__()\n",
    "        self.history = {}\n",
    "\n",
    "        self.net = net\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metrics_dict = nn.ModuleDict(metrics_dict)\n",
    "\n",
    "        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(\n",
    "            self.parameters(), lr=1e-2)\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.net:\n",
    "            return self.net.forward(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    # 训练模型的方法\n",
    "    def fit(self, train_data, val_data=None, epochs=10, ckpt_path='checkpoint.pt',\n",
    "            patience=5, monitor=\"val_loss\", mode=\"min\"):\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
    "\n",
    "            # 1. 训练阶段\n",
    "            train_step_runner = StepRunner(net=self.net, stage=\"train\",\n",
    "                                           loss_fn=self.loss_fn, metrics_dict=deepcopy(self.metrics_dict),\n",
    "                                           optimizer=self.optimizer, lr_scheduler=self.lr_scheduler)\n",
    "            train_epoch_runner = EpochRunner(train_step_runner)\n",
    "            train_metrics = train_epoch_runner(train_data)\n",
    "\n",
    "            for name, metric in train_metrics.items():\n",
    "                self.history[name] = self.history.get(name, []) + [metric]\n",
    "\n",
    "            # 2. 验证阶段\n",
    "            if val_data:\n",
    "                val_step_runner = StepRunner(net=self.net, stage=\"val\",\n",
    "                                             loss_fn=self.loss_fn, metrics_dict=deepcopy(self.metrics_dict))\n",
    "                val_epoch_runner = EpochRunner(val_step_runner)\n",
    "                with torch.no_grad():\n",
    "                    val_metrics = val_epoch_runner(val_data)\n",
    "                val_metrics[\"epoch\"] = epoch\n",
    "                for name, metric in val_metrics.items():\n",
    "                    self.history[name] = self.history.get(name, []) + [metric]\n",
    "\n",
    "            # 3. 提前停止\n",
    "            if not val_data:\n",
    "                continue\n",
    "            arr_scores = self.history[monitor]\n",
    "            best_score_idx = np.argmax(arr_scores) if mode == \"max\" else np.argmin(arr_scores)\n",
    "            if best_score_idx == len(arr_scores) - 1:\n",
    "                torch.save(self.net.state_dict(), ckpt_path)\n",
    "                print(\"<<<<<< reach best {0} : {1} >>>>>>\".format(monitor,\n",
    "                                                                  arr_scores[best_score_idx]), file=sys.stderr)\n",
    "            if len(arr_scores) - best_score_idx > patience:\n",
    "                print(\"<<<<<< {} without improvement in {} epoch, early stopping >>>>>>\".format(\n",
    "                    monitor, patience), file=sys.stderr)\n",
    "                break\n",
    "\n",
    "        self.net.load_state_dict(torch.load(ckpt_path))\n",
    "        return pd.DataFrame(self.history)\n",
    "\n",
    "    # 评估模型性能的方法\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, val_data):\n",
    "        val_step_runner = StepRunner(net=self.net, stage=\"val\",\n",
    "                                     loss_fn=self.loss_fn, metrics_dict=deepcopy(self.metrics_dict))\n",
    "        val_epoch_runner = EpochRunner(val_step_runner)\n",
    "        val_metrics = val_epoch_runner(val_data)\n",
    "        return val_metrics\n",
    "\n",
    "    # 使用模型进行预测的方法\n",
    "    @torch.no_grad()\n",
    "    def predict(self, dataloader):\n",
    "        self.net.eval()\n",
    "        result = torch.cat([self.forward(t[0]) for t in dataloader])\n",
    "        return result.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "# 创建一个新的神经网络模型net\n",
    "net = Net()\n",
    "\n",
    "# 使用KerasModel类构建一个模型对象model，传入以下参数：\n",
    "# - net: 神经网络模型\n",
    "# - loss_fn: 损失函数，这里使用二元交叉熵损失函数 (nn.BCEWithLogitsLoss())\n",
    "# - optimizer: 优化器，这里使用Adam优化器 (torch.optim.Adam) 并传入网络参数和学习率\n",
    "# - metrics_dict: 度量指标字典，包含一个二元分类准确率度量指标 (Accuracy)\n",
    "model = KerasModel(net,\n",
    "                   loss_fn=nn.BCEWithLogitsLoss(),\n",
    "                   optimizer=torch.optim.Adam(net.parameters(), lr=0.01),\n",
    "                   metrics_dict={\"acc\": Accuracy(task='binary')}\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a04882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用模型对训练数据集 dl_train 进行训练\n",
    "# - val_data: 验证数据集 dl_val，用于验证模型性能\n",
    "# - epochs: 训练的总周期数\n",
    "# - ckpt_path: 用于保存模型检查点的路径\n",
    "# - patience: 提前停止的耐心，如果连续指定的度量没有改善，则提前停止训练\n",
    "# - monitor: 监控的度量指标，在这里是验证集的准确率 'val_acc'\n",
    "# - mode: 模式，'max' 表示监控的度量指标越大越好\n",
    "model.fit(dl_train,\n",
    "          val_data=dl_val,\n",
    "          epochs=10,\n",
    "          ckpt_path='checkpoint',\n",
    "          patience=3,\n",
    "          monitor='val_acc',\n",
    "          mode='max')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff35951",
   "metadata": {},
   "source": [
    "### 四，评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 pandas 库\n",
    "import pandas as pd\n",
    "\n",
    "# 从模型对象 model 中获取训练过程中的历史记录\n",
    "history = model.history\n",
    "\n",
    "# 创建一个 DataFrame 对象 dfhistory，将历史记录存储在其中\n",
    "dfhistory = pd.DataFrame(history)\n",
    "\n",
    "# 打印 DataFrame，显示训练过程中的指标值和损失\n",
    "dfhistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93107420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 matplotlib.pyplot 库，用于绘图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 定义一个函数 plot_metric，用于绘制训练和验证指标曲线\n",
    "def plot_metric(dfhistory, metric):\n",
    "    # 从历史记录 DataFrame 中获取训练和验证指标的数据\n",
    "    train_metrics = dfhistory[\"train_\" + metric]\n",
    "    val_metrics = dfhistory['val_' + metric]\n",
    "\n",
    "    # 获取训练周期数（x轴）\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "\n",
    "    # 绘制训练指标和验证指标的曲线\n",
    "    plt.plot(epochs, train_metrics, 'bo--')  # 蓝色圆点线表示训练指标\n",
    "    plt.plot(epochs, val_metrics, 'ro-')  # 红色实线表示验证指标\n",
    "\n",
    "    # 设置图表标题、x轴标签、y轴标签和图例\n",
    "    plt.title('Training and validation ' + metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\" + metric, 'val_' + metric])\n",
    "\n",
    "    # 显示图表\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51029d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(dfhistory, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94294cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(dfhistory, \"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a148942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估\n",
    "evaluation_result = model.evaluate(dl_val)\n",
    "print(evaluation_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc62f0",
   "metadata": {},
   "source": [
    "### 五，使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a67857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个用于模型预测的函数 predict\n",
    "def predict(net, dl):\n",
    "    # 将模型设置为评估模式\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        # 对数据加载器中的数据进行预测，并通过Sigmoid函数进行概率化\n",
    "        result = nn.Sigmoid()(torch.cat([net.forward(t[0]) for t in dl]))\n",
    "    return result.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f916311",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = predict(net, dl_val)\n",
    "y_pred_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c888a8",
   "metadata": {},
   "source": [
    "### 六，保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee13dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型权重已经被保存在了ckpt_path='checkpoint.'\n",
    "net_clone = Net()\n",
    "net_clone.load_state_dict(torch.load('checkpoint'))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
