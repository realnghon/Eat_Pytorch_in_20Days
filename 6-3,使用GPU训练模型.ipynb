{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-3,使用GPU训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习的训练过程常常非常耗时，一个模型训练几个小时是家常便饭，训练几天也是常有的事情，有时候甚至要训练几十天。\n",
    "\n",
    "训练过程的耗时主要来自于两个部分，一部分来自数据准备，另一部分来自参数迭代。\n",
    "\n",
    "当数据准备过程还是模型训练时间的主要瓶颈时，我们可以使用更多进程来准备数据。\n",
    "\n",
    "当参数迭代过程成为训练时间的主要瓶颈时，我们通常的方法是应用GPU来进行加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q torchkeras\n",
    "!pip install -q  -U torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:21:37.080604Z",
     "iopub.status.busy": "2023-08-02T09:21:37.079630Z",
     "iopub.status.idle": "2023-08-02T09:21:44.145337Z",
     "shell.execute_reply": "2023-08-02T09:21:44.144019Z",
     "shell.execute_reply.started": "2023-08-02T09:21:37.080551Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchkeras\n",
    "import torchmetrics\n",
    "\n",
    "print(\"torch.__version__ = \", torch.__version__)\n",
    "print(\"torchkeras.__version__ = \", torchkeras.__version__)\n",
    "print(\"torchmetrics.__version__ = \", torchmetrics.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：本节代码只能在有GPU的机器环境上才能正确执行。\n",
    "\n",
    "对于没有GPU的同学，推荐使用\n",
    "\n",
    "在Colab笔记本中：修改->笔记本设置->硬件加速器 中选择 GPU\n",
    "\n",
    "可点击如下链接，直接在kaggle中运行范例代码。\n",
    "\n",
    "https://www.kaggle.com/lyhue1991/pytorch-gpu-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pytorch中使用GPU加速模型非常简单，只要将模型和数据移动到GPU上。核心代码只有以下几行。\n",
    "\n",
    "```python\n",
    "# 定义模型\n",
    "... \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # 移动模型到cuda\n",
    "\n",
    "# 训练模型\n",
    "...\n",
    "\n",
    "features = features.to(device) # 移动数据到cuda\n",
    "labels = labels.to(device) # 或者  labels = labels.cuda() if torch.cuda.is_available() else labels\n",
    "...\n",
    "```\n",
    "\n",
    "如果要使用多个GPU训练模型，也非常简单。只需要在将模型设置为数据并行风格模型。\n",
    "则模型移动到GPU上之后，会在每一个GPU上拷贝一个副本，并把数据平分到各个GPU上进行训练。核心代码如下。\n",
    "\n",
    "```python\n",
    "# 定义模型\n",
    "... \n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model) # 包装为并行风格模型\n",
    "\n",
    "# 训练模型\n",
    "...\n",
    "features = features.to(device) # 移动数据到cuda\n",
    "labels = labels.to(device) # 或者 labels = labels.cuda() if torch.cuda.is_available() else labels\n",
    "...\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 〇，GPU相关操作汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 1，查看gpu信息\n",
    "if_cuda = torch.cuda.is_available()\n",
    "print(\"if_cuda=\", if_cuda)\n",
    "\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(\"gpu_count=\", gpu_count)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:21:50.949675Z",
     "iopub.status.busy": "2023-08-02T09:21:50.949297Z",
     "iopub.status.idle": "2023-08-02T09:21:55.584912Z",
     "shell.execute_reply": "2023-08-02T09:21:55.583660Z",
     "shell.execute_reply.started": "2023-08-02T09:21:50.949642Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建一个随机初始化的100x100张量（tensor）\n",
    "tensor = torch.rand((100, 100))\n",
    "\n",
    "# 将张量移到GPU上\n",
    "tensor_gpu = tensor.to(\"cuda:0\")  # 或者使用 tensor_gpu = tensor.cuda()\n",
    "# - to方法可以将张量移到指定的设备（这里是GPU的第一个设备cuda:0）\n",
    "# - 使用.to(\"cuda:0\") 或 .cuda() 方法是将张量移动到GPU的两种常见方式之一\n",
    "# - tensor_gpu 保存了移动到GPU上的张量\n",
    "\n",
    "# 打印张量所在设备信息\n",
    "print(tensor_gpu.device)\n",
    "# - 打印 tensor_gpu 的设备信息，应该显示为 cuda:0，表示在GPU上\n",
    "\n",
    "# 检查张量是否在GPU上\n",
    "print(tensor_gpu.is_cuda)\n",
    "# - 打印 tensor_gpu 是否位于GPU上的布尔值，应该为True，表示张量在GPU上\n",
    "\n",
    "# 将张量从GPU移到CPU上\n",
    "tensor_cpu = tensor_gpu.to(\"cpu\")  # 或者使用 tensor_cpu = tensor_gpu.cpu()\n",
    "# - to方法可以将张量从一个设备移动到另一个设备，这里是从GPU移动到CPU\n",
    "# - 使用.to(\"cpu\") 或 .cpu() 方法是将张量从GPU移动到CPU的两种常见方式之一\n",
    "# - tensor_cpu 保存了从GPU移到CPU上的张量\n",
    "\n",
    "# 打印张量所在设备信息\n",
    "print(tensor_cpu.device)\n",
    "# - 打印 tensor_cpu 的设备信息，应该显示为 cpu，表示张量在CPU上\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:21:55.587734Z",
     "iopub.status.busy": "2023-08-02T09:21:55.587050Z",
     "iopub.status.idle": "2023-08-02T09:21:55.597566Z",
     "shell.execute_reply": "2023-08-02T09:21:55.596260Z",
     "shell.execute_reply.started": "2023-08-02T09:21:55.587689Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建一个简单的神经网络模型，这里是一个线性层，输入维度为2，输出维度为1\n",
    "net = nn.Linear(2, 1)\n",
    "\n",
    "# 打印模型的第一个参数张量是否位于GPU上\n",
    "print(next(net.parameters()).is_cuda)\n",
    "# - next(net.parameters()) 获取模型的第一个参数张量\n",
    "# - .is_cuda 属性用于检查张量是否位于GPU上\n",
    "# - 输出应该为False，因为模型还没有移动到GPU上\n",
    "\n",
    "# 将模型中的全部参数张量移动到GPU上\n",
    "net.to(\"cuda:0\")\n",
    "# - 使用 .to() 方法将模型中的全部参数张量依次移动到GPU上，这会改变模型本身的设备\n",
    "# - 注意，不需要重新赋值为 net = net.to(\"cuda:0\")，模型本身已经在GPU上了\n",
    "\n",
    "# 打印模型的第一个参数张量是否位于GPU上\n",
    "print(next(net.parameters()).is_cuda)\n",
    "# - 打印模型的第一个参数张量是否位于GPU上的布尔值，应该为True，表示模型参数已经在GPU上\n",
    "\n",
    "# 打印模型的第一个参数张量所在设备信息\n",
    "print(next(net.parameters()).device)\n",
    "# - 打印模型的第一个参数张量所在设备信息，应该显示为 cuda:0，表示参数在GPU上\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:21:55.599678Z",
     "iopub.status.busy": "2023-08-02T09:21:55.599312Z",
     "iopub.status.idle": "2023-08-02T09:21:55.621575Z",
     "shell.execute_reply": "2023-08-02T09:21:55.620550Z",
     "shell.execute_reply.started": "2023-08-02T09:21:55.599640Z"
    }
   },
   "outputs": [],
   "source": [
    "# 引入PyTorch库中的神经网络模块\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个简单的线性模型，输入维度为2，输出维度为1\n",
    "linear = nn.Linear(2, 1)\n",
    "\n",
    "# 打印模型的第一个参数张量所在设备信息\n",
    "print(next(linear.parameters()).device)\n",
    "# - next(linear.parameters()) 获取模型的第一个参数张量\n",
    "# - .device 属性用于检查张量所在的设备，通常在CPU上\n",
    "\n",
    "# 使用 nn.DataParallel 将模型支持多个GPU数据并行\n",
    "model = nn.DataParallel(linear)\n",
    "# - nn.DataParallel 可以将模型包装起来，以支持多个GPU数据并行\n",
    "# - 这意味着可以在多个GPU上同时处理不同的数据批次，以加速训练\n",
    "\n",
    "# 打印模型的设备IDs，即支持数据并行的GPU设备列表\n",
    "print(model.device_ids)\n",
    "# - 打印模型的设备IDs，应该显示为 [0]，表示模型在GPU 0 上进行数据并行\n",
    "# - 通常，这个列表会包含多个GPU的设备ID，以便在多个GPU上并行运算\n",
    "\n",
    "# 打印模型的第一个参数张量所在设备信息\n",
    "print(next(model.module.parameters()).device)\n",
    "# - next(model.module.parameters()) 获取模型中的第一个参数张量\n",
    "# - .module 用于访问 nn.DataParallel 中包装的模型\n",
    "# - .parameters() 用于获取模型的参数张量\n",
    "# - 打印参数张量所在设备信息，应该显示为 cuda:0，表示参数在GPU 0 上\n",
    "\n",
    "# 注意：当保存模型参数时，需要指定保存 model.module 的参数\n",
    "torch.save(model.module.state_dict(), \"model_parameter.pt\")\n",
    "\n",
    "# 创建一个新的线性模型\n",
    "linear = nn.Linear(2, 1)\n",
    "\n",
    "# 加载保存的模型参数\n",
    "linear.load_state_dict(torch.load(\"model_parameter.pt\"))\n",
    "# - 使用 .load_state_dict() 方法加载保存的模型参数\n",
    "# - 这样可以在新的模型上使用之前训练好的参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一，矩阵乘法范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面分别使用CPU和GPU作一个矩阵乘法，并比较其计算效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:21:58.132801Z",
     "iopub.status.busy": "2023-08-02T09:21:58.131981Z",
     "iopub.status.idle": "2023-08-02T09:21:58.138927Z",
     "shell.execute_reply": "2023-08-02T09:21:58.137799Z",
     "shell.execute_reply.started": "2023-08-02T09:21:58.132746Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:21:59.222992Z",
     "iopub.status.busy": "2023-08-02T09:21:59.222619Z",
     "iopub.status.idle": "2023-08-02T09:21:59.871529Z",
     "shell.execute_reply": "2023-08-02T09:21:59.870275Z",
     "shell.execute_reply.started": "2023-08-02T09:21:59.222960Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建两个随机初始化的矩阵 a 和 b\n",
    "a = torch.rand((10000, 200))\n",
    "b = torch.rand((200, 10000))\n",
    "\n",
    "# 记录开始时间\n",
    "tic = time.time()\n",
    "\n",
    "# 执行矩阵乘法运算\n",
    "c = torch.matmul(a, b)\n",
    "\n",
    "# 记录结束时间\n",
    "toc = time.time()\n",
    "\n",
    "# 打印矩阵乘法运算所花费的时间\n",
    "print(toc - tic)\n",
    "\n",
    "# 打印矩阵 a 和 b 的设备信息\n",
    "print(a.device)\n",
    "print(b.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:22:01.349894Z",
     "iopub.status.busy": "2023-08-02T09:22:01.349166Z",
     "iopub.status.idle": "2023-08-02T09:22:02.226856Z",
     "shell.execute_reply": "2023-08-02T09:22:02.224728Z",
     "shell.execute_reply.started": "2023-08-02T09:22:01.349856Z"
    }
   },
   "outputs": [],
   "source": [
    "# 引入PyTorch库\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# 检查GPU是否可用，如果可用，将使用GPU，否则使用CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 在指定设备上创建随机矩阵 a\n",
    "a = torch.rand((10000, 200), device=device)\n",
    "\n",
    "# 在CPU上创建随机矩阵 b，然后将其移动到与矩阵 a 相同的设备上\n",
    "b = torch.rand((200, 10000))\n",
    "b = b.to(device)  # 或者使用 b = b.cuda() if torch.cuda.is_available() else b\n",
    "\n",
    "# 记录开始时间\n",
    "tic = time.time()\n",
    "\n",
    "# 执行矩阵乘法运算\n",
    "c = torch.matmul(a, b)\n",
    "\n",
    "# 记录结束时间\n",
    "toc = time.time()\n",
    "\n",
    "# 打印矩阵乘法运算所花费的时间\n",
    "print(toc - tic)\n",
    "\n",
    "# 打印矩阵 a 和 b 的设备信息\n",
    "print(a.device)\n",
    "print(b.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二，线性回归范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T14:58:49.525724Z",
     "iopub.status.busy": "2022-07-17T14:58:49.525304Z",
     "iopub.status.idle": "2022-07-17T14:58:49.546334Z",
     "shell.execute_reply": "2022-07-17T14:58:49.544588Z",
     "shell.execute_reply.started": "2022-07-17T14:58:49.525694Z"
    }
   },
   "source": [
    "下面对比使用CPU和GPU训练一个线性回归模型的效率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1，使用CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:22:05.233030Z",
     "iopub.status.busy": "2023-08-02T09:22:05.232639Z",
     "iopub.status.idle": "2023-08-02T09:22:05.297141Z",
     "shell.execute_reply": "2023-08-02T09:22:05.296102Z",
     "shell.execute_reply.started": "2023-08-02T09:22:05.232997Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入PyTorch库\n",
    "import torch\n",
    "\n",
    "# 准备数据\n",
    "n = 1000000  # 样本数量\n",
    "\n",
    "# 生成一个形状为 (n, 2) 的随机张量 X，其值在 [-5.0, 5.0) 之间，服从均匀分布\n",
    "X = 10 * torch.rand([n, 2]) - 5.0\n",
    "\n",
    "# 创建一个权重张量 w0，形状为 (1, 2)，用于线性组合\n",
    "w0 = torch.tensor([[2.0, -3.0]])\n",
    "\n",
    "# 创建一个偏置张量 b0，形状为 (1, 1)，用于线性组合\n",
    "b0 = torch.tensor([[10.0]])\n",
    "\n",
    "# 生成一个形状为 (n, 1) 的目标值张量 Y\n",
    "# 通过将 X 与 w0 的转置相乘，再加上 b0，并添加正态分布噪声，来生成目标值\n",
    "# @ 表示矩阵乘法，将 X 与 w0 的转置相乘，得到形状为 (n, 1) 的结果\n",
    "# 然后，加上偏置 b0，最后添加一个服从均值为 0，标准差为 2.0 的正态分布噪声\n",
    "Y = X @ w0.t() + b0 + torch.normal(0.0, 2.0, size=[n, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:22:06.677267Z",
     "iopub.status.busy": "2023-08-02T09:22:06.676242Z",
     "iopub.status.idle": "2023-08-02T09:22:06.685850Z",
     "shell.execute_reply": "2023-08-02T09:22:06.684746Z",
     "shell.execute_reply.started": "2023-08-02T09:22:06.677187Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入PyTorch库中的 nn 模块\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 定义线性回归模型类\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 创建模型参数 w 和 b，这些参数将在训练中学习\n",
    "        # w 初始值是从与 w0 形状相同的正态分布中随机抽取的\n",
    "        self.w = nn.Parameter(torch.randn_like(w0))\n",
    "\n",
    "        # b 初始值是与 b0 形状相同的全零张量\n",
    "        self.b = nn.Parameter(torch.zeros_like(b0))\n",
    "\n",
    "    # 正向传播方法，定义了如何计算模型的输出\n",
    "    def forward(self, x):\n",
    "        # 计算预测值，x 与参数 w 的转置相乘再加上参数 b\n",
    "        return x @ self.w.t() + self.b\n",
    "\n",
    "\n",
    "# 创建一个 LinearRegression 类的实例化对象 linear，该对象代表一个线性回归模型\n",
    "linear = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:22:08.718803Z",
     "iopub.status.busy": "2023-08-02T09:22:08.718085Z",
     "iopub.status.idle": "2023-08-02T09:22:13.949452Z",
     "shell.execute_reply": "2023-08-02T09:22:13.948304Z",
     "shell.execute_reply.started": "2023-08-02T09:22:08.718765Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch.optim as optim  # 导入优化器\n",
    "import time  # 导入时间模块\n",
    "\n",
    "# 定义优化器，使用 Adam 优化器来更新模型参数\n",
    "# 参数 linear.parameters() 包含模型的权重和偏置参数\n",
    "# lr 表示学习率，控制每次参数更新的步长\n",
    "optimizer = optim.Adam(linear.parameters(), lr=0.1)\n",
    "\n",
    "# 定义损失函数，使用均方误差损失函数 (Mean Squared Error)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "# 定义训练函数\n",
    "def train(epochs):\n",
    "    tic = time.time()  # 记录训练开始时间\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()  # 清零梯度，避免梯度累积\n",
    "\n",
    "        # 计算模型对训练数据 X 的预测值\n",
    "        Y_pred = linear(X)\n",
    "\n",
    "        # 计算预测值与真实值 Y 之间的均方误差损失\n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "\n",
    "        # 反向传播，计算梯度\n",
    "        loss.backward()\n",
    "\n",
    "        # 使用优化器来更新模型参数，实际执行梯度下降步骤\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每训练 50 个 epoch 打印一次损失值\n",
    "        if epoch % 50 == 0:\n",
    "            print({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "\n",
    "    toc = time.time()  # 记录训练结束时间\n",
    "    print(\"time used:\", toc - tic)  # 打印训练用时\n",
    "\n",
    "\n",
    "# 调用 train 函数，训练模型 500 个 epoch\n",
    "train(500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2，使用GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:22:13.952419Z",
     "iopub.status.busy": "2023-08-02T09:22:13.951355Z",
     "iopub.status.idle": "2023-08-02T09:22:13.998524Z",
     "shell.execute_reply": "2023-08-02T09:22:13.997457Z",
     "shell.execute_reply.started": "2023-08-02T09:22:13.952376Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入PyTorch库\n",
    "import torch\n",
    "\n",
    "# 准备数据\n",
    "n = 1000000  # 样本数量\n",
    "\n",
    "# 生成一个形状为 (n, 2) 的张量 X，其中每个元素都在 -5.0 到 5.0 之间均匀分布\n",
    "X = 10 * torch.rand([n, 2]) - 5.0\n",
    "\n",
    "# 创建一个权重张量 w0，形状为 (1, 2)，用于线性变换\n",
    "w0 = torch.tensor([[2.0, -3.0]])\n",
    "\n",
    "# 创建一个偏置项张量 b0，形状为 (1, 1)\n",
    "b0 = torch.tensor([[10.0]])\n",
    "\n",
    "# 生成目标标签 Y，通过执行线性变换 X @ w0.t() + b0，并添加正态分布的噪声\n",
    "# X @ w0.t() 表示 X 与 w0 的矩阵乘法，得到一个形状为 (n, 1) 的结果\n",
    "# 然后，b0 被加到每个样本上\n",
    "# 最后，torch.normal(0.0, 2.0, size=[n, 1]) 生成均值为 0.0，标准差为 2.0 的正态分布噪声\n",
    "Y = X @ w0.t() + b0 + torch.normal(0.0, 2.0, size=[n, 1])\n",
    "\n",
    "# 数据移动到GPU上\n",
    "# 首先，检查是否可用GPU\n",
    "print(\"torch.cuda.is_available() = \", torch.cuda.is_available())\n",
    "\n",
    "# 如果GPU可用，将张量 X 和 Y 移动到GPU上进行加速计算\n",
    "if torch.cuda.is_available():\n",
    "    X = X.cuda()\n",
    "    Y = Y.cuda()\n",
    "\n",
    "# 打印设备信息，以确认数据是否成功移动到GPU\n",
    "print(\"X.device:\", X.device)  # 打印 X 张量所在的设备\n",
    "print(\"Y.device:\", Y.device)  # 打印 Y 张量所在的设备\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:22:14.856923Z",
     "iopub.status.busy": "2023-08-02T09:22:14.856519Z",
     "iopub.status.idle": "2023-08-02T09:22:14.867761Z",
     "shell.execute_reply": "2023-08-02T09:22:14.866595Z",
     "shell.execute_reply.started": "2023-08-02T09:22:14.856887Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入PyTorch库中的 nn 模块\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 定义线性回归模型\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 定义模型的权重参数 w，使用 nn.Parameter 包装张量，初始化为与 w0 形状相同的随机张量\n",
    "        self.w = nn.Parameter(torch.randn_like(w0))\n",
    "\n",
    "        # 定义模型的偏置参数 b，使用 nn.Parameter 包装张量，初始化为与 b0 形状相同的全零张量\n",
    "        self.b = nn.Parameter(torch.zeros_like(b0))\n",
    "\n",
    "    # 定义正向传播方法，计算模型的输出\n",
    "    def forward(self, x):\n",
    "        return x @ self.w.t() + self.b\n",
    "\n",
    "\n",
    "# 创建线性回归模型实例\n",
    "linear = LinearRegression()\n",
    "\n",
    "# 移动模型到GPU上（如果GPU可用的话）\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 使用 .to() 方法将模型移动到指定的设备（GPU或CPU）\n",
    "linear.to(device)\n",
    "\n",
    "# 查看模型是否已经移动到GPU上，通过检查模型参数中的第一个参数的 .is_cuda 属性\n",
    "# 如果第一个参数位于GPU上，那么整个模型通常也会在GPU上执行\n",
    "print(\"if on cuda:\", next(linear.parameters()).is_cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:22:21.232004Z",
     "iopub.status.busy": "2023-08-02T09:22:21.231614Z",
     "iopub.status.idle": "2023-08-02T09:22:21.785143Z",
     "shell.execute_reply": "2023-08-02T09:22:21.783907Z",
     "shell.execute_reply.started": "2023-08-02T09:22:21.231970Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入所需的时间模块\n",
    "import time\n",
    "\n",
    "# 定义优化器，使用 Adam 优化器来更新模型参数\n",
    "# 参数 linear.parameters() 返回模型的可学习参数（包括 w 和 b）\n",
    "# lr=0.1 是学习率的设置\n",
    "optimizer = torch.optim.Adam(linear.parameters(), lr=0.1)\n",
    "\n",
    "# 定义损失函数，使用均方误差损失 (MSE Loss)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "# 定义训练函数，接受一个参数 epoches，表示训练的总轮数\n",
    "def train(epoches):\n",
    "    tic = time.time()\n",
    "    for epoch in range(epoches):\n",
    "        # 每轮训练前先将梯度清零，以免梯度累积\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 通过模型进行正向传播，计算预测值 Y_pred\n",
    "        Y_pred = linear(X)\n",
    "\n",
    "        # 计算预测值 Y_pred 与真实标签 Y 之间的均方误差损失\n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "\n",
    "        # 反向传播，计算梯度\n",
    "        loss.backward()\n",
    "\n",
    "        # 使用优化器更新模型参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每隔 50 轮打印一次当前轮数和损失\n",
    "        if epoch % 50 == 0:\n",
    "            print({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "\n",
    "    toc = time.time()\n",
    "    print(\"time used:\", toc - tic)\n",
    "\n",
    "\n",
    "# 调用训练函数进行模型训练，总共训练 500 轮\n",
    "train(500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三，图片分类范例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:22:24.911548Z",
     "iopub.status.busy": "2023-08-02T09:22:24.911130Z",
     "iopub.status.idle": "2023-08-02T09:22:24.917106Z",
     "shell.execute_reply": "2023-08-02T09:22:24.915927Z",
     "shell.execute_reply.started": "2023-08-02T09:22:24.911513Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入PyTorch库\n",
    "import torch\n",
    "\n",
    "# 导入PyTorch的图像处理工具包\n",
    "import torchvision\n",
    "\n",
    "# 从torchvision模块导入transforms，transforms包含了图像预处理和数据增强的功能\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:22:26.656218Z",
     "iopub.status.busy": "2023-08-02T09:22:26.655272Z",
     "iopub.status.idle": "2023-08-02T09:22:27.893139Z",
     "shell.execute_reply": "2023-08-02T09:22:27.892010Z",
     "shell.execute_reply.started": "2023-08-02T09:22:26.656155Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入PyTorch库\n",
    "import torch\n",
    "\n",
    "# 导入PyTorch的计算机视觉工具包\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义数据预处理操作，这里使用transforms.Compose()将多个预处理操作组合在一起\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# 创建训练集数据集对象\n",
    "# - root: 数据集存储的根目录\n",
    "# - train=True: 表示加载训练集数据\n",
    "# - download=True: 如果数据不存在，会自动下载\n",
    "# - transform=transform: 使用之前定义的数据预处理操作\n",
    "ds_train = torchvision.datasets.MNIST(root=\"mnist/\", train=True, download=True, transform=transform)\n",
    "\n",
    "# 创建验证集数据集对象\n",
    "# - root: 数据集存储的根目录\n",
    "# - train=False: 表示加载验证集数据\n",
    "# - download=True: 如果数据不存在，会自动下载\n",
    "# - transform=transform: 使用之前定义的数据预处理操作\n",
    "ds_val = torchvision.datasets.MNIST(root=\"mnist/\", train=False, download=True, transform=transform)\n",
    "\n",
    "# 创建训练集数据加载器\n",
    "# - ds_train: 训练集数据集对象\n",
    "# - batch_size=128: 每个批次中包含128个样本\n",
    "# - shuffle=True: 在每个时代(epoch)开始时打乱数据\n",
    "# - num_workers=2: 使用2个工作进程来加载数据，加速数据加载\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# 创建验证集数据加载器\n",
    "# - ds_val: 验证集数据集对象\n",
    "# - batch_size=128: 每个批次中包含128个样本\n",
    "# - shuffle=False: 不需要在验证集上打乱数据\n",
    "# - num_workers=2: 使用2个工作进程来加载数据，加速数据加载\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# 打印训练集和验证集的样本数量\n",
    "print(len(ds_train))\n",
    "print(len(ds_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:22:30.365184Z",
     "iopub.status.busy": "2023-08-02T09:22:30.364138Z",
     "iopub.status.idle": "2023-08-02T09:22:30.380062Z",
     "shell.execute_reply": "2023-08-02T09:22:30.378908Z",
     "shell.execute_reply.started": "2023-08-02T09:22:30.365148Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个函数用于创建神经网络模型\n",
    "def create_net():\n",
    "    # 创建一个神经网络模型，使用Sequential容器来构建一个层序列\n",
    "    net = nn.Sequential()\n",
    "\n",
    "    # 添加卷积层1，输入通道数为1（MNIST图像是单通道的灰度图像），输出通道数为32，卷积核大小为3x3\n",
    "    net.add_module(\"conv1\", nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3))\n",
    "\n",
    "    # 添加最大池化层1，池化核大小为2x2，步幅为2\n",
    "    net.add_module(\"pool1\", nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    # 添加卷积层2，输入通道数为32，输出通道数为64，卷积核大小为5x5\n",
    "    net.add_module(\"conv2\", nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5))\n",
    "\n",
    "    # 添加最大池化层2，池化核大小为2x2，步幅为2\n",
    "    net.add_module(\"pool2\", nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    # 添加2D Dropout层，以防止过拟合，丢弃率为0.1\n",
    "    net.add_module(\"dropout\", nn.Dropout2d(p=0.1))\n",
    "\n",
    "    # 添加自适应最大池化层，将特征图大小调整为(1, 1)\n",
    "    net.add_module(\"adaptive_pool\", nn.AdaptiveMaxPool2d((1, 1)))\n",
    "\n",
    "    # 添加Flatten层，将特征图展平为一维向量\n",
    "    net.add_module(\"flatten\", nn.Flatten())\n",
    "\n",
    "    # 添加全连接层1，输入特征数为64，输出特征数为32\n",
    "    net.add_module(\"linear1\", nn.Linear(64, 32))\n",
    "\n",
    "    # 添加ReLU激活函数\n",
    "    net.add_module(\"relu\", nn.ReLU())\n",
    "\n",
    "    # 添加全连接层2，输入特征数为32，输出特征数为10（对应于10个类别的输出）\n",
    "    net.add_module(\"linear2\", nn.Linear(32, 10))\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "# 调用create_net函数创建神经网络模型\n",
    "net = create_net()\n",
    "\n",
    "# 打印神经网络模型的结构\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1，使用CPU进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:22:53.737134Z",
     "iopub.status.busy": "2023-08-02T09:22:53.736757Z",
     "iopub.status.idle": "2023-08-02T09:24:50.589851Z",
     "shell.execute_reply": "2023-08-02T09:24:50.588548Z",
     "shell.execute_reply.started": "2023-08-02T09:22:53.737099Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入所需的库和模块\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "# 定义一个用于打印日志的函数\n",
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\" + \"==========\" * 8 + \"%s\" % nowtime)\n",
    "    print(str(info) + \"\\n\")\n",
    "\n",
    "\n",
    "# 创建神经网络模型（这里假设create_net()函数已经在前面定义过）\n",
    "net = create_net()\n",
    "\n",
    "# 定义损失函数（交叉熵损失）\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器（Adam优化器）\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# 定义用于评估模型性能的指标（准确率）\n",
    "metrics_dict = {\"acc\": Accuracy(task='multiclass', num_classes=10)}\n",
    "\n",
    "# 训练轮数\n",
    "epochs = 3\n",
    "\n",
    "# 检查点保存路径\n",
    "ckpt_path = 'checkpoint.pt'\n",
    "\n",
    "# 提前停止（early stopping）相关设置\n",
    "monitor = \"val_acc\"\n",
    "patience = 1\n",
    "mode = \"max\"\n",
    "\n",
    "# 用于保存训练历史的字典\n",
    "history = {}\n",
    "\n",
    "# 开始训练循环\n",
    "for epoch in range(1, epochs + 1):\n",
    "    printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
    "\n",
    "    # 1. 训练\n",
    "    net.train()\n",
    "\n",
    "    total_loss, step = 0, 0\n",
    "\n",
    "    # 使用tqdm创建一个进度条以显示训练进度\n",
    "    loop = tqdm(enumerate(dl_train), total=len(dl_train), file=sys.stdout)\n",
    "    train_metrics_dict = deepcopy(metrics_dict)\n",
    "\n",
    "    for i, batch in loop:\n",
    "        features, labels = batch\n",
    "\n",
    "        # 前向传播\n",
    "        preds = net(features)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 计算指标（例如，准确率）\n",
    "        step_metrics = {\"train_\" + name: metric_fn(preds, labels).item()\n",
    "                        for name, metric_fn in train_metrics_dict.items()}\n",
    "\n",
    "        step_log = dict({\"train_loss\": loss.item()}, **step_metrics)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        step += 1\n",
    "        if i != len(dl_train) - 1:\n",
    "            loop.set_postfix(**step_log)\n",
    "        else:\n",
    "            epoch_loss = total_loss / step\n",
    "            epoch_metrics = {\"train_\" + name: metric_fn.compute().item()\n",
    "                             for name, metric_fn in train_metrics_dict.items()}\n",
    "            epoch_log = dict({\"train_loss\": epoch_loss}, **epoch_metrics)\n",
    "            loop.set_postfix(**epoch_log)\n",
    "\n",
    "            for name, metric_fn in train_metrics_dict.items():\n",
    "                metric_fn.reset()\n",
    "\n",
    "    for name, metric in epoch_log.items():\n",
    "        history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "    # 2. 验证\n",
    "    net.eval()\n",
    "\n",
    "    total_loss, step = 0, 0\n",
    "    loop = tqdm(enumerate(dl_val), total=len(dl_val), file=sys.stdout)\n",
    "    val_metrics_dict = deepcopy(metrics_dict)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in loop:\n",
    "            features, labels = batch\n",
    "\n",
    "            # 前向传播\n",
    "            preds = net(features)\n",
    "            loss = loss_fn(preds, labels)\n",
    "\n",
    "            # 计算指标\n",
    "            step_metrics = {\"val_\" + name: metric_fn(preds, labels).item()\n",
    "                            for name, metric_fn in val_metrics_dict.items()}\n",
    "\n",
    "            step_log = dict({\"val_loss\": loss.item()}, **step_metrics)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            step += 1\n",
    "            if i != len(dl_val) - 1:\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_loss = (total_loss / step)\n",
    "                epoch_metrics = {\"val_\" + name: metric_fn.compute().item()\n",
    "                                 for name, metric_fn in val_metrics_dict.items()}\n",
    "                epoch_log = dict({\"val_loss\": epoch_loss}, **epoch_metrics)\n",
    "                loop.set_postfix(**epoch_log)\n",
    "\n",
    "                for name, metric_fn in val_metrics_dict.items():\n",
    "                    metric_fn.reset()\n",
    "\n",
    "    epoch_log[\"epoch\"] = epoch\n",
    "    for name, metric in epoch_log.items():\n",
    "        history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "    # 3. 提前停止\n",
    "    arr_scores = history[monitor]\n",
    "    best_score_idx = np.argmax(arr_scores) if mode == \"max\" else np.argmin(arr_scores)\n",
    "    if best_score_idx == len(arr_scores) - 1:\n",
    "        torch.save(net.state_dict(), ckpt_path)\n",
    "        print(\"<<<<<< reach best {0} : {1} >>>>>>\".format(monitor,\n",
    "                                                          arr_scores[best_score_idx]))\n",
    "    if len(arr_scores) - best_score_idx > patience:\n",
    "        print(\"<<<<<< {} without improvement in {} epoch, early stopping >>>>>>\".format(\n",
    "            monitor, patience))\n",
    "        break\n",
    "    net.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "# 将训练历史保存为DataFrame\n",
    "dfhistory = pd.DataFrame(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2，使用GPU进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:27:40.875092Z",
     "iopub.status.busy": "2023-08-02T09:27:40.874670Z",
     "iopub.status.idle": "2023-08-02T09:28:18.788065Z",
     "shell.execute_reply": "2023-08-02T09:28:18.786947Z",
     "shell.execute_reply.started": "2023-08-02T09:27:40.875053Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入所需的库和模块\n",
    "import sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "# 定义一个用于打印日志的函数\n",
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\" + \"==========\" * 8 + \"%s\" % nowtime)\n",
    "    print(str(info) + \"\\n\")\n",
    "\n",
    "\n",
    "# 创建神经网络模型（假设create_net()函数已在之前定义过）\n",
    "net = create_net()\n",
    "\n",
    "# 定义损失函数（交叉熵损失）\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器（Adam优化器）\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# 定义用于评估模型性能的指标（准确率）\n",
    "metrics_dict = {\"acc\": Accuracy(task='multiclass', num_classes=10)}\n",
    "\n",
    "# 移动模型和相关组件到GPU（如果可用）\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "loss_fn.to(device)\n",
    "for name, fn in metrics_dict.items():\n",
    "    fn.to(device)\n",
    "\n",
    "# 训练轮数\n",
    "epochs = 5\n",
    "\n",
    "# 检查点保存路径\n",
    "ckpt_path = 'checkpoint.pt'\n",
    "\n",
    "# 提前停止（early stopping）相关设置\n",
    "monitor = \"val_acc\"\n",
    "patience = 1\n",
    "mode = \"max\"\n",
    "\n",
    "# 用于保存训练历史的字典\n",
    "history = {}\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
    "\n",
    "    # 1. 训练\n",
    "    net.train()\n",
    "\n",
    "    total_loss, step = 0, 0\n",
    "\n",
    "    # 使用tqdm创建一个进度条以显示训练进度\n",
    "    loop = tqdm(enumerate(dl_train), total=len(dl_train), file=sys.stdout)\n",
    "    train_metrics_dict = deepcopy(metrics_dict)\n",
    "\n",
    "    for i, batch in loop:\n",
    "        features, labels = batch\n",
    "\n",
    "        # 移动数据到GPU上\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        preds = net(features)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 计算指标（例如，准确率）\n",
    "        step_metrics = {\"train_\" + name: metric_fn(preds, labels).item()\n",
    "                        for name, metric_fn in train_metrics_dict.items()}\n",
    "\n",
    "        step_log = dict({\"train_loss\": loss.item()}, **step_metrics)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        step += 1\n",
    "        if i != len(dl_train) - 1:\n",
    "            loop.set_postfix(**step_log)\n",
    "        else:\n",
    "            epoch_loss = total_loss / step\n",
    "            epoch_metrics = {\"train_\" + name: metric_fn.compute().item()\n",
    "                             for name, metric_fn in train_metrics_dict.items()}\n",
    "            epoch_log = dict({\"train_loss\": epoch_loss}, **epoch_metrics)\n",
    "            loop.set_postfix(**epoch_log)\n",
    "\n",
    "            for name, metric_fn in train_metrics_dict.items():\n",
    "                metric_fn.reset()\n",
    "\n",
    "    for name, metric in epoch_log.items():\n",
    "        history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "    # 2. 验证\n",
    "    net.eval()\n",
    "\n",
    "    total_loss, step = 0, 0\n",
    "    loop = tqdm(enumerate(dl_val), total=len(dl_val), file=sys.stdout)\n",
    "    val_metrics_dict = deepcopy(metrics_dict)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in loop:\n",
    "            features, labels = batch\n",
    "\n",
    "            # 移动数据到GPU上\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            preds = net(features)\n",
    "            loss = loss_fn(preds, labels)\n",
    "\n",
    "            # 计算指标\n",
    "            step_metrics = {\"val_\" + name: metric_fn(preds, labels).item()\n",
    "                            for name, metric_fn in val_metrics_dict.items()}\n",
    "\n",
    "            step_log = dict({\"val_loss\": loss.item()}, **step_metrics)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            step += 1\n",
    "            if i != len(dl_val) - 1:\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_loss = (total_loss / step)\n",
    "                epoch_metrics = {\"val_\" + name: metric_fn.compute().item()\n",
    "                                 for name, metric_fn in val_metrics_dict.items()}\n",
    "                epoch_log = dict({\"val_loss\": epoch_loss}, **epoch_metrics)\n",
    "                loop.set_postfix(**epoch_log)\n",
    "\n",
    "                for name, metric_fn in val_metrics_dict.items():\n",
    "                    metric_fn.reset()\n",
    "\n",
    "    epoch_log[\"epoch\"] = epoch\n",
    "    for name, metric in epoch_log.items():\n",
    "        history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "    # 3. 提前停止\n",
    "    arr_scores = history[monitor]\n",
    "    best_score_idx = np.argmax(arr_scores) if mode == \"max\" else np.argmin(arr_scores)\n",
    "    if best_score_idx == len(arr_scores) - 1:\n",
    "        torch.save(net.state_dict(), ckpt_path)\n",
    "        print(\"<<<<<< reach best {0} : {1} >>>>>>\".format(monitor,\n",
    "                                                          arr_scores[best_score_idx]))\n",
    "    if len(arr_scores) - best_score_idx > patience:\n",
    "        print(\"<<<<<< {} without improvement in {} epoch, early stopping >>>>>>\".format(\n",
    "            monitor, patience))\n",
    "        break\n",
    "    net.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "# 将训练历史保存为DataFrame\n",
    "dfhistory = pd.DataFrame(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四，torchkeras.KerasModel中使用GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的例子可以看到，在pytorch中使用GPU并不复杂，但对于经常炼丹的同学来说，模型和数据老是移来移去还是蛮麻烦的。\n",
    "\n",
    "一不小心就会忘了移动某些数据或者某些module，导致报错。\n",
    "\n",
    "torchkeras.KerasModel 在设计的时候考虑到了这一点，如果环境当中存在可用的GPU，会自动使用GPU，反之则使用CPU。\n",
    "\n",
    "通过引入accelerate的一些基础功能，torchkeras.KerasModel以非常优雅的方式在GPU和CPU之间切换。\n",
    "\n",
    "详细实现可以参考torchkeras.KerasModel的源码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:31:06.804407Z",
     "iopub.status.busy": "2023-08-02T09:31:06.803922Z",
     "iopub.status.idle": "2023-08-02T09:31:06.819012Z",
     "shell.execute_reply": "2023-08-02T09:31:06.817856Z",
     "shell.execute_reply.started": "2023-08-02T09:31:06.804365Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入 accelerate 库\n",
    "import accelerate\n",
    "\n",
    "# 创建 Accelerator 实例，它将帮助自动选择合适的硬件加速器（例如 GPU）\n",
    "accelerator = accelerate.Accelerator()\n",
    "\n",
    "# 打印加速器设备信息\n",
    "print(accelerator.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T09:31:12.618443Z",
     "iopub.status.busy": "2023-08-02T09:31:12.618036Z",
     "iopub.status.idle": "2023-08-02T09:32:49.031087Z",
     "shell.execute_reply": "2023-08-02T09:32:49.029676Z",
     "shell.execute_reply.started": "2023-08-02T09:31:12.618408Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# 从 torchkeras 库中导入 KerasModel 类\n",
    "from torchkeras import KerasModel\n",
    "\n",
    "# 导入需要的库和模块\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# 创建神经网络模型\n",
    "net = create_net()\n",
    "\n",
    "# 使用 KerasModel 类创建模型，设置损失函数、指标（准确率）、优化器等参数\n",
    "model = KerasModel(net,\n",
    "                   loss_fn=nn.CrossEntropyLoss(),\n",
    "                   metrics_dict={\"acc\": Accuracy(task='multiclass', num_classes=10)},\n",
    "                   optimizer=torch.optim.Adam(net.parameters(), lr=0.01))\n",
    "\n",
    "# 使用 fit() 方法来进行模型训练\n",
    "model.fit(\n",
    "    train_data=dl_train,  # 训练数据集\n",
    "    val_data=dl_val,  # 验证数据集\n",
    "    epochs=10,  # 训练轮数\n",
    "    patience=3,  # 提前停止的耐心轮数\n",
    "    monitor=\"val_acc\",  # 监控的性能指标（这里是验证准确率）\n",
    "    mode=\"max\"  # 监控指标的模式（最大化验证准确率）\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
